{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bab6a139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from libpysal.weights import Queen\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv, GATConv, GraphConv, TransformerConv\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool, global_add_pool\n",
    "from torch_geometric.nn import BatchNorm, LayerNorm\n",
    "import math\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import root_mean_squared_error, mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric\n",
    "from torch_geometric.explain import Explainer, GNNExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98727616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SULAWESI TENGAH', 'SULAWESI BARAT', 'SULAWESI SELATAN', 'PAPUA BARAT', 'GORONTALO', 'RIAU', 'DAERAH ISTIMEWA YOGYAKARTA', 'SUMATERA BARAT', 'DKI JAKARTA', 'MALUKU', 'BENGKULU', 'LAMPUNG', 'PAPUA', 'KEPULAUAN RIAU', 'NUSA TENGGARA BARAT', 'JAMBI', 'BALI', 'JAWA TIMUR', 'SUMATERA UTARA', 'SULAWESI TENGGARA', 'NUSA TENGGARA TIMUR', 'KALIMANTAN SELATAN', 'ACEH', 'KALIMANTAN TENGAH', 'KEPULAUAN BANGKA BELITUNG', 'SUMATERA SELATAN', 'BANTEN', 'SULAWESI UTARA', 'KALIMANTAN UTARA', 'KALIMANTAN TIMUR', 'JAWA TENGAH', 'MALUKU UTARA', 'KALIMANTAN BARAT', 'JAWA BARAT']\n"
     ]
    }
   ],
   "source": [
    "adj = pd.read_excel('adjacency_geografi.xlsx', index_col=0)\n",
    "province_order = adj.columns.tolist()\n",
    "for i, province in enumerate(province_order):\n",
    "    province_order[i] = province.upper()\n",
    "\n",
    "print(province_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38f23756",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Sorted.xlsx')\n",
    "\n",
    "years = df['Tahun'].unique()\n",
    "\n",
    "similarity_matrices = {}\n",
    "\n",
    "for year in years:\n",
    "    df_year = df[df['Tahun'] == year].copy()\n",
    "\n",
    "    df_year.set_index('Provinsi', inplace=True)\n",
    "\n",
    "    input_feature = df_year.drop(columns=['Unnamed: 0.1', 'Unnamed: 0', 'Tahun', 'Stunting'])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    feature_scaled = scaler.fit_transform(input_feature)\n",
    "\n",
    "    similarity_matrix = cosine_similarity(feature_scaled)\n",
    "\n",
    "    similarity_df = pd.DataFrame(similarity_matrix,\n",
    "                                 index=input_feature.index,\n",
    "                                 columns=input_feature.index)\n",
    "    \n",
    "    similarity_matrices[year] = similarity_df\n",
    "\n",
    "    similarity_df.to_excel(f\"Similarity_Matrix_for_{year}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70ddf431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_edge_index_from_similarity(sim_matrix, threshold=0.85):\n",
    "    edges = []\n",
    "    n = sim_matrix.shape[0]\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j and sim_matrix[i, j] >= threshold:\n",
    "                edges.append([i, j])\n",
    "    return torch.tensor(edges).t().contiguous()  # shape [2, num_edges]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17e7ff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = pd.read_excel('adjacency_geografi.xlsx', index_col=0)\n",
    "\n",
    "# Buat mapping provinsi\n",
    "province_list = adj.columns.tolist()\n",
    "province_to_idx = {prov.upper(): idx for idx, prov in enumerate(province_list)}\n",
    "\n",
    "# Buat edge_index\n",
    "edge_index = []\n",
    "\n",
    "for i, source in enumerate(province_list):\n",
    "    for j, target in enumerate(province_list):\n",
    "        if adj.iloc[i, j] == 1:\n",
    "            edge_index.append([i, j])  # <--- harus pakai i, j dari enumerate (bukan nilai dari file langsung)\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).T  # shape [2, num_edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5162d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hansc\\AppData\\Local\\Temp\\ipykernel_31784\\538702535.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index_spatial = torch.tensor(edge_index, dtype=torch.long)  # shape [2, num_edges]\n",
      "C:\\Users\\hansc\\AppData\\Local\\Temp\\ipykernel_31784\\538702535.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index_spatial = torch.tensor(edge_index, dtype=torch.long)  # shape [2, num_edges]\n",
      "C:\\Users\\hansc\\AppData\\Local\\Temp\\ipykernel_31784\\538702535.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index_spatial = torch.tensor(edge_index, dtype=torch.long)  # shape [2, num_edges]\n",
      "C:\\Users\\hansc\\AppData\\Local\\Temp\\ipykernel_31784\\538702535.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index_spatial = torch.tensor(edge_index, dtype=torch.long)  # shape [2, num_edges]\n",
      "C:\\Users\\hansc\\AppData\\Local\\Temp\\ipykernel_31784\\538702535.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index_spatial = torch.tensor(edge_index, dtype=torch.long)  # shape [2, num_edges]\n",
      "C:\\Users\\hansc\\AppData\\Local\\Temp\\ipykernel_31784\\538702535.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index_spatial = torch.tensor(edge_index, dtype=torch.long)  # shape [2, num_edges]\n"
     ]
    }
   ],
   "source": [
    "graphs_spatial = []\n",
    "graphs_similarity = []\n",
    "\n",
    "for tahun in years:\n",
    "    # ambil fitur dan target provinsi di tahun ini\n",
    "    fitur_tahun = df[df['Tahun'] == tahun].drop(columns=['Unnamed: 0.1', 'Unnamed: 0', 'Provinsi', 'Tahun', 'Stunting'])\n",
    "    target_tahun = df[df['Tahun'] == tahun]['Stunting']\n",
    "    sim_matrix = similarity_matrices[tahun].values # N x N matrix\n",
    "    \n",
    "    x = torch.tensor(fitur_tahun.values, dtype=torch.float)\n",
    "    y = torch.tensor(target_tahun.values, dtype=torch.float)\n",
    "    \n",
    "    # adjacency edge index (tetap)\n",
    "    edge_index_spatial = torch.tensor(edge_index, dtype=torch.long)  # shape [2, num_edges]\n",
    "    \n",
    "    # similarity edge index (dinamis)\n",
    "    edge_index_sim = get_edge_index_from_similarity(sim_matrix, threshold=0.85)\n",
    "\n",
    "    # buat 2 graph data terpisah\n",
    "    data_spatial = Data(x=x, edge_index=edge_index_spatial)\n",
    "    data_similarity = Data(x=x, edge_index=edge_index_sim)\n",
    "\n",
    "    # simpan\n",
    "    graphs_spatial.append(data_spatial)\n",
    "    graphs_similarity.append(data_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e891e96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Spacial: [Data(x=[34, 8], edge_index=[2, 62]), Data(x=[34, 8], edge_index=[2, 62]), Data(x=[34, 8], edge_index=[2, 62]), Data(x=[34, 8], edge_index=[2, 62]), Data(x=[34, 8], edge_index=[2, 62]), Data(x=[34, 8], edge_index=[2, 62])]\n",
      "Graph Similarity: [Data(x=[34, 8], edge_index=[2, 906]), Data(x=[34, 8], edge_index=[2, 906]), Data(x=[34, 8], edge_index=[2, 904]), Data(x=[34, 8], edge_index=[2, 968]), Data(x=[34, 8], edge_index=[2, 938]), Data(x=[34, 8], edge_index=[2, 958])]\n"
     ]
    }
   ],
   "source": [
    "print(f'Graph Spacial: {graphs_spatial}')\n",
    "\n",
    "print(f'Graph Similarity: {graphs_similarity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a20ae077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tahun 2018\n",
      "Spatial edge max: 33\n",
      "Similarity edge max: 33\n",
      "Tahun 2019\n",
      "Spatial edge max: 33\n",
      "Similarity edge max: 33\n",
      "Tahun 2020\n",
      "Spatial edge max: 33\n",
      "Similarity edge max: 33\n",
      "Tahun 2021\n",
      "Spatial edge max: 33\n",
      "Similarity edge max: 33\n",
      "Tahun 2022\n",
      "Spatial edge max: 33\n",
      "Similarity edge max: 33\n",
      "Tahun 2023\n",
      "Spatial edge max: 33\n",
      "Similarity edge max: 33\n"
     ]
    }
   ],
   "source": [
    "for year, (g1, g2) in enumerate(zip(graphs_spatial, graphs_similarity)):\n",
    "    print(f\"Tahun {year + 2018}\")\n",
    "    print(\"Spatial edge max:\", g1.edge_index.max().item())\n",
    "    print(\"Similarity edge max:\", g2.edge_index.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af28fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DualBranchGNN(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # Branch 1: Spatial Graph\n",
    "#         self.conv_spatial_1 = SAGEConv(in_channels, hidden_channels)\n",
    "#         self.norm_spatial_1 = torch.nn.LayerNorm(hidden_channels)\n",
    "        \n",
    "#         self.conv_spatial_2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "#         self.norm_spatial_2 = torch.nn.LayerNorm(hidden_channels)\n",
    "        \n",
    "#         # Branch 2: Similarity Graph\n",
    "#         self.conv_sim_1 = SAGEConv(in_channels, hidden_channels)\n",
    "#         self.norm_sim_1 = torch.nn.LayerNorm(hidden_channels)\n",
    "        \n",
    "#         self.conv_sim_2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "#         self.norm_sim_2 = torch.nn.LayerNorm(hidden_channels)\n",
    "        \n",
    "#         # Output layer\n",
    "#         self.out_layer = torch.nn.Linear(hidden_channels * 2, out_channels)\n",
    "\n",
    "#     def forward(self, data_spatial, data_sim):\n",
    "#         # Spatial branch\n",
    "#         x1 = self.conv_spatial_1(data_spatial.x, data_spatial.edge_index)\n",
    "#         x1 = self.norm_spatial_1(x1)\n",
    "#         x1 = F.relu(x1)\n",
    "        \n",
    "#         x1 = self.conv_spatial_2(x1, data_spatial.edge_index)\n",
    "#         x1 = self.norm_spatial_2(x1)\n",
    "#         x1 = F.relu(x1)\n",
    "        \n",
    "#         # Similarity branch\n",
    "#         x2 = self.conv_sim_1(data_sim.x, data_sim.edge_index)\n",
    "#         x2 = self.norm_sim_1(x2)\n",
    "#         x2 = F.relu(x2)\n",
    "        \n",
    "#         x2 = self.conv_sim_2(x2, data_sim.edge_index)\n",
    "#         x2 = self.norm_sim_2(x2)\n",
    "#         x2 = F.relu(x2)\n",
    "        \n",
    "#         # Concatenate\n",
    "#         x = torch.cat([x1, x2], dim=1)\n",
    "#         out = self.out_layer(x)\n",
    "#         return out\n",
    "\n",
    "#     def get_embedding(self, data_spatial, data_sim):\n",
    "#         # Spatial branch\n",
    "#         x1 = self.conv_spatial_1(data_spatial.x, data_spatial.edge_index)\n",
    "#         x1 = self.norm_spatial_1(x1)\n",
    "#         x1 = F.relu(x1)\n",
    "        \n",
    "#         x1 = self.conv_spatial_2(x1, data_spatial.edge_index)\n",
    "#         x1 = self.norm_spatial_2(x1)\n",
    "#         x1 = F.relu(x1)\n",
    "        \n",
    "#         # Similarity branch\n",
    "#         x2 = self.conv_sim_1(data_sim.x, data_sim.edge_index)\n",
    "#         x2 = self.norm_sim_1(x2)\n",
    "#         x2 = F.relu(x2)\n",
    "        \n",
    "#         x2 = self.conv_sim_2(x2, data_sim.edge_index)\n",
    "#         x2 = self.norm_sim_2(x2)\n",
    "#         x2 = F.relu(x2)\n",
    "        \n",
    "#         # Concatenate tanpa output layer\n",
    "#         x = torch.cat([x1, x2], dim=1)  # Ini hasil embedding gabungan\n",
    "#         return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2c17e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DualBranchGNN(in_channels=8, hidden_channels=256, out_channels=1)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "# loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# best_loss = float('inf')\n",
    "# best_model_state = None\n",
    "\n",
    "# for epoch in range(1000):\n",
    "#     model.train()\n",
    "#     total_loss = 0\n",
    "\n",
    "#     for year in range(6):  # 6 tahun\n",
    "#         data_spatial = graphs_spatial[year]\n",
    "#         data_sim = graphs_similarity[year]\n",
    "#         y_true = data_spatial.y.view(-1, 1)  # Target\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         out = model(data_spatial, data_sim)\n",
    "#         loss = loss_fn(out, y_true)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#     # Simpan model jika loss lebih baik\n",
    "#     if total_loss < best_loss:\n",
    "#         best_loss = total_loss\n",
    "#         best_model_state = model.state_dict()  # Save best weights\n",
    "\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(f\"Epoch {epoch} - Loss: {total_loss:.4f} {'(BEST)' if total_loss == best_loss else ''}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "384e2f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(best_model_state)\n",
    "# model.eval()\n",
    "# all_preds = []\n",
    "# all_targets = []\n",
    "# gnn_embeddings_all = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for spatial_data, sim_data in zip(graphs_spatial, graphs_similarity):\n",
    "#         # Kirim kedua data ke model\n",
    "#         out = model(spatial_data, sim_data)\n",
    "\n",
    "#         embedding = model.get_embedding(spatial_data, sim_data)\n",
    "#         gnn_embeddings_all.append(embedding)\n",
    "\n",
    "#         y_pred = out.squeeze().cpu().numpy()\n",
    "#         y_true = spatial_data.y.squeeze().cpu().numpy()  # atau sim_data.y, sama saja\n",
    "\n",
    "#         all_preds.extend(y_pred)\n",
    "#         all_targets.extend(y_true)\n",
    "\n",
    "\n",
    "# gnn_embeddings_all = torch.cat(gnn_embeddings_all, dim=0)  # [204, hidden_dim*2]\n",
    "# print(gnn_embeddings_all.shape)\n",
    "\n",
    "# for true, pred in zip(all_targets, all_preds):\n",
    "#     print(f'True Value: {round(float(true), 2)}, Predicted: {round(float(pred), 2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82e85179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv = GCNConv(in_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c45c7249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models import DeepGraphInfomax\n",
    "\n",
    "def corruption(x, edge_index):\n",
    "    # Shuffle fitur node (negatif sampel)\n",
    "    return x[torch.randperm(x.size(0))], edge_index\n",
    "\n",
    "def create_dgi_model(in_channels, hidden_channels):\n",
    "    encoder = GCNEncoder(in_channels, hidden_channels)\n",
    "    dgi = DeepGraphInfomax(\n",
    "        hidden_channels=hidden_channels,\n",
    "        encoder=encoder,\n",
    "        summary=lambda z, *args, **kwargs: torch.sigmoid(z.mean(dim=0)),\n",
    "        corruption=corruption\n",
    "    )\n",
    "    return dgi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "002de50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dgi(dgi, data, epochs=200, lr=0.001):\n",
    "    optimizer = torch.optim.AdamW(dgi.parameters(), lr=lr)\n",
    "    dgi.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        pos_z, neg_z, summary = dgi(data.x, data.edge_index)\n",
    "        loss = dgi.loss(pos_z, neg_z, summary)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    dgi.eval()\n",
    "    with torch.no_grad():\n",
    "        z, _, _ = dgi(data.x, data.edge_index)\n",
    "    return z  # [34, hidden_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "217e42ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tahun 0\n",
      "Tahun 1\n",
      "Tahun 2\n",
      "Tahun 3\n",
      "Tahun 4\n",
      "Tahun 5\n"
     ]
    }
   ],
   "source": [
    "all_embeddings = []\n",
    "\n",
    "for tahun in range(len(graphs_spatial)):\n",
    "    print(f\"Tahun {tahun}\")\n",
    "    \n",
    "    # Spatial Graph\n",
    "    dgi_spatial = create_dgi_model(in_channels=8, hidden_channels=64)\n",
    "    emb_spatial = train_dgi(dgi_spatial, graphs_spatial[tahun])\n",
    "    \n",
    "    # Similarity Graph\n",
    "    dgi_sim = create_dgi_model(in_channels=8, hidden_channels=64)\n",
    "    emb_sim = train_dgi(dgi_sim, graphs_similarity[tahun])\n",
    "    \n",
    "    # Concatenate [34, 32]\n",
    "    emb_combined = torch.cat([emb_spatial, emb_sim], dim=1)  # [34, 128]\n",
    "\n",
    "    # Ambil rata-rata vektor embedding per tahun (agregasi node → 1 vektor per graph)\n",
    "    graph_embedding = emb_combined.mean(dim=0)  # [128]\n",
    "\n",
    "    all_embeddings.append(graph_embedding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92db2e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['SD / Sederajat', 'SMP / Sederajat', 'SMA / Sederajat',\n",
      "       'Prevalensi Ketidakcukupan Konsumsi Pangan (Persen)',\n",
      "       'Konsumsi Kalori Per Hari Per Kapita',\n",
      "       'Konsumsi Protein Per Hari Per Kapita', 'Kemisikinan Maret',\n",
      "       'Akses Sanitasi Bersih'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "nama_fitur = fitur_tahun.columns\n",
    "print(nama_fitur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0978388e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 128)\n"
     ]
    }
   ],
   "source": [
    "from tabpfn import TabPFNRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "x_gnn = np.stack([np.array(e) for e in all_embeddings])\n",
    "print(x_gnn.shape) \n",
    "\n",
    "# Load dan preprocessing# Load dan preprocessing\n",
    "df = pd.read_excel('Sorted.xlsx')\n",
    "df = df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0', 'SMP / Sederajat', 'SMA / Sederajat'])  # jangan hapus 'Provinsi' di sini\n",
    "\n",
    "# Mapping tahun ke index embedding\n",
    "unique_years = sorted(df['Tahun'].unique())\n",
    "year_to_index = {year: i for i, year in enumerate(unique_years)}\n",
    "\n",
    "# Expand embedding ke semua baris sesuai tahun\n",
    "x_gnn_expanded = np.array([x_gnn[year_to_index[year]] for year in df['Tahun']])\n",
    "\n",
    "# Drop kolom non-numerik\n",
    "X = df.drop(columns=['Stunting', 'Provinsi'])  # Sekarang ini valid\n",
    "\n",
    "# Skala\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Gabungkan\n",
    "X_combined = np.concatenate([x_gnn_expanded, X_scaled], axis=1)\n",
    "\n",
    "# Target\n",
    "y = df['Stunting']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_combined, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8702c057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tahun\n",
      "2018     34\n",
      "2019     34\n",
      "2020     34\n",
      "2021     34\n",
      "2022     34\n",
      "2023     34\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[['Tahun']].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c5fe80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil cross-validation disimpan di 'rf_cv_mae_scores.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Inisialisasi model\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=152,\n",
    "    max_depth=19,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features=None,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# K-Fold setup\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Cross-validation scoring\n",
    "scores = cross_val_score(rf_model, X_combined, y, scoring='neg_mean_absolute_error', cv=kf)\n",
    "mae_scores = -scores\n",
    "\n",
    "# Simpan ke file CSV\n",
    "df_scores = pd.DataFrame({\n",
    "    'Fold': [f'Fold_{i+1}' for i in range(len(mae_scores))],\n",
    "    'MAE': mae_scores\n",
    "})\n",
    "df_scores.loc[len(df_scores.index)] = ['Mean', mae_scores.mean()]  # Tambahkan rata-rata\n",
    "df_scores.to_csv('rf_GNN64.csv', index=False)\n",
    "\n",
    "print(\"Hasil cross-validation disimpan di 'rf_cv_mae_scores.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational_biology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
